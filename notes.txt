batch_vb_samp_boost
num_it, abs_grad,  F, P, Trees, nr_wts (number remained after Weight Trimming for samples), nr_wtc (... after ... classes)


batch_vb2_samp_boost
num_it, abs_grad,  F, P, Trees, nr_wts, nr_wtc, tree_node_cc


batch_vb3_samp_boost
num_it, abs_grad,  F, P, Trees, nr_wts, nr_wtc, tree_node_cc, tree_node_sc (cc: class count, sc: sample count)

batch_vb4_samp_boost
output: num_it, abs_grad,  F, P, Trees, tree_node_cc, tree_node_sc (nr_xxx can be infered from tree_node_xxx), tree_is_leaf, GradCls, LossCls
input: rs,wrs, rf, rc,wrc (rs: ratio sample, wrs: weight ratio sample, rf: ratio feature, rc: ratio classes, wrc: weight ratio classes )


batch_vb5_samp_boost
output: num_it, abs_grad,  F, P, Trees, tree_node_cc, tree_node_sc (nr_xxx can be infered from tree_node_xxx), tree_is_leaf
input: rs,wrs, rf, rc,wrc (rs: ratio sample, wrs: weight ratio sample, rf: ratio feature, rc: ratio classes, wrc: weight ratio classes )

batch_CoSampboost_basic
output: num_it, abs_grad,  F, P, Trees, tree_node_cc, tree_node_sc (nr_xxx can be infered from tree_node_xxx), tree_is_leaf
input: rb, wrb, rf  (rb: ratio budget, wrb: weight ratio budget, rf: ratio feature)


pSampVTLogitBoost
VTLogitBoost with uniform sampling for samples, features and classes

pExtSampVTLogitBoost
VTLogitBoost, but does Extreme Sampling for features: each node uniformly samples features

pExtSamp2VTLogitBoost
pExtSampVTLogitBoost, but use weight trimming for samples, where weight is simply the absolute value of gradient. rs is weight ratio

pExtSamp3VTLogitBoost
pExtSampVTLogitBoost, but use Friedman's weight trimming for samples, i.e. p*(1-p)

pExtSamp4VTLogitBoost
pExtSampVTLogitBoost, but use weight trimming for samples, where the weight is the Newton decrement = (r-p)^2 / (p*(1-p))

pVbExtSamp5VTLogitBoost
pExtSampVTLogitBoost, but use weight trimming for both samples and classes, where the weight is simply the absolute value of gradient. Verbose version

pVbExtSamp6VTLogitBoost
pVbExtSamp5VTLogitBoost, but the parameter "rs" means ratio of number of examples, instead of weights.

pVbExtSamp7VTLogitBoost
pVbExtSamp3VTLogitBoost (i.e. Friedman's weight trimming where weight is p*(1-p) ), but the parameter "rs" means ratio of number of examples, 
instead of weights. 

pVbExtSamp8VTLogitBoost
pVbExtSamp6VTLogitBoost, but do the Class Weight Trimming for each node. rc is the class count ratio.

pVbExtSamp9VTLogitBoost
pVbExtSamp8VTLogitBoost, but use all classes when fitting node values.

pVbExtSamp10VTLogitBoost
pVbExtSamp9VTLogitBoost, but rc is the weight sum ratio (instead of class count ratio). The selected class count is recorded for each node.

pVbExtSamp11VTLogitBoost
pVbExtSamp10VTLogitBoost, but record also #examples for each node (to see the actual computation needed); set minimum #classes after sampling to 2.
parameters: rs,wrs, rf, rc,wrc

pVbExtSamp12VTLogitBoost
see batch_vb4_samp_boost for its parameters

pVbExtSamp13VTLogitBoost
pVbExtSamp12VTLogitBoost, but record the loss & the gradients of each class. See batch_vb5_samp_boost.

pCoSampVTLogitBoost
Sample-Class co-sampling. see batch_CoSampboost_basic.

